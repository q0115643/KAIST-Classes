{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import classify\n",
    "from gensim.models import Word2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data files.\n",
    "    The returned object contains various fields that store the data.\n",
    "    The data is preprocessed for use with scikit-learn.\n",
    "    \n",
    "    Description of each fileds of returned object are as follows.\n",
    "    \n",
    "    - count_vec: CountVectorizer used to process the data (for reapplication on new data)\n",
    "    - trainX,devX,testX,unlabeledX: Array of vectors representing Bags of Words, i.e. documents processed through the vectorizer\n",
    "    - le: LabelEncoder, i.e. a mapper from string labels to ints (stored for reapplication)\n",
    "    - target_labels: List of labels (same order as used in le)\n",
    "    - trainy,devy: Array of int labels, one for each document\n",
    "    - test_fnames: List of test file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v(speech, vocab, size, window, epochs, load_w2v):\n",
    "    if load_w2v:\n",
    "        load_w2v(speech)\n",
    "    else:\n",
    "        train_w2v(speech, vocab, size, window, epochs)\n",
    "    speech.trainX = speech.trainX.A\n",
    "    speech.testX = speech.testX.A\n",
    "    speech.devX = speech.devX.A\n",
    "    speech.train_doc_vec = classify.get_doc_vec(speech.trainX, speech.word_vectors)\n",
    "    speech.test_doc_vec = classify.get_doc_vec(speech.testX, speech.word_vectors)\n",
    "    speech.dev_doc_vec = classify.get_doc_vec(speech.devX, speech.word_vectors)\n",
    "    #speech.unlabeled_doc_vec = word2vec_vectorize.get_doc_vec(speech.unlabeledX, speech.glove_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_w2v(speech):\n",
    "    # 로드\n",
    "    w2vFileName = 'w2v.p'\n",
    "    w2vFilePath = os.path.join('w2v_learned', w2vFileName)\n",
    "    with open(w2vFilePath, \"rb\") as w2vFile:\n",
    "        speech.word_vectors = pickle.load(w2vFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_w2v(speech):\n",
    "    # 저장\n",
    "    w2vFileName = 'w2v.p'\n",
    "    w2vFilePath = os.path.join('w2v_learned', w2vFileName)\n",
    "    with open(w2vFilePath, 'wb') as w2vFile:\n",
    "        pickle.dump(speech.word_vectors, w2vFile)\n",
    "        print('saved file: ' + w2vFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_w2v(speech, vocab, size, window, epochs):\n",
    "    model = Word2Vec(size=size, window=window, min_count=1, workers=8, iter=epochs, sg=1, sorted_vocab=0)\n",
    "    model.build_vocab(vocab)\n",
    "    # speech.train_data 가 들어온다.\n",
    "    docs = speech.train_data\n",
    "    model.train(docs, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    speech.word_vectors = model.wv.syn0\n",
    "    print(\"word2vec 생성 완료: \" + str(speech.word_vectors.shape))\n",
    "    save_w2v(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(data_loc, size, window, epochs, load_w2v):\n",
    "    class Data: pass\n",
    "    speech = Data()\n",
    "\n",
    "    print(\"-- train data\")\n",
    "    speech.train_data, speech.train_fnames, speech.train_labels = read_tsv(data_loc, \"train.tsv\")\n",
    "    print(len(speech.train_data))\n",
    "\n",
    "    print(\"-- dev data\")\n",
    "    speech.dev_data, speech.dev_fnames, speech.dev_labels = read_tsv(data_loc, \"dev.tsv\")\n",
    "    print(len(speech.dev_data))\n",
    "\n",
    "    print(\"-- test data\")\n",
    "    speech.test_data, speech.test_fnames = read_unlabeled(data_loc, 'test')\n",
    "\n",
    "    #print(\"-- unlabeled data\")\n",
    "    #unlabeled_data, unlabeled_fnames = read_unlabeled(data_loc, 'unlabeled')\n",
    "    #print(len(unlabeled_fnames))\n",
    "\n",
    "    print(\"-- transforming data and labels\")\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    for word2vec method\n",
    "    '''\n",
    "    speech.count_vect = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    speech.trainX = speech.count_vect.fit_transform(speech.train_data)\n",
    "    speech.vocab = speech.count_vect.get_feature_names()\n",
    "    speech.vocab = [[word] for word in speech.vocab]\n",
    "\n",
    "    print(\"trainX 생성 완료 shape: \" + str(speech.trainX.shape))\n",
    "    speech.devX = speech.count_vect.transform(speech.dev_data)\n",
    "    speech.testX = speech.count_vect.transform(speech.test_data)\n",
    "    #speech.unlabeledX = speech.count_vect.transform(unlabeled_data)\n",
    "    \n",
    "    get_w2v(speech, speech.vocab, size, window, epochs, load_w2v)\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    speech.le = preprocessing.LabelEncoder()\n",
    "    speech.le.fit(speech.train_labels)\n",
    "    speech.target_labels = speech.le.classes_\n",
    "    speech.trainy = speech.le.transform(speech.train_labels)\n",
    "    speech.devy = speech.le.transform(speech.dev_labels)\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tsv(data_loc, fname):\n",
    "    \"\"\"Reads the labeled data described in tsv file.\n",
    "    The returned object contains three fields that represent the unlabeled data.\n",
    "    data: documents, each document is represented as list of words\n",
    "    fnames: list of filenames, one for each document\n",
    "    labels: list of labels for each document (List of string)\n",
    "    \"\"\"\n",
    "    tf = codecs.open(data_loc + fname, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    labels = []\n",
    "    fnames = []\n",
    "    for line in tf:\n",
    "        (ifname, label) = line.strip().split(\"\\t\")\n",
    "        content = read_instance(data_loc, ifname)\n",
    "        labels.append(label)\n",
    "        fnames.append(ifname)\n",
    "        data.append(content)\n",
    "    tf.close()\n",
    "    return data, fnames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_unlabeled(data_loc, dname):\n",
    "    \"\"\"Reads the unlabeled data.\n",
    "    The returned object contains two fields that represent the unlabeled data.\n",
    "    data: documents, each document is represented as list of words\n",
    "    fnames: list of filenames, one for each document\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    fnames = []\n",
    "    raw_fnames = os.listdir(data_loc+dname)\n",
    "    for raw_fname in raw_fnames:\n",
    "        fname = dname + '/' + raw_fname\n",
    "        content = read_instance(data_loc, fname)\n",
    "        data.append(content)\n",
    "        fnames.append(fname)\n",
    "    return data, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_instance(data_loc, ifname):\n",
    "    \"\"\"Reads the document file.\n",
    "    Each document file has a string represents sequence of words, \n",
    "    and each words are seperated by semicolon.\n",
    "    This function convert this string into list of words and return it.\n",
    "    \"\"\"\n",
    "    inst = data_loc + ifname\n",
    "    ifile = codecs.open(inst, 'r', encoding='utf-8')\n",
    "    content = ifile.read().strip()\n",
    "    content = content.split(';')\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_pred_kaggle_file(cls, outfname, speech):\n",
    "    \"\"\"Writes the predictions in Kaggle format.\n",
    "\n",
    "    Given the classifier, output filename, and the speech object,\n",
    "    this function write the predictions of the classifier on the test data and\n",
    "    writes it to the outputfilename. \n",
    "    \"\"\"\n",
    "    yp = cls.predict(speech.test_doc_vec)\n",
    "    labels = speech.le.inverse_transform(yp)\n",
    "    f = codecs.open(outfname, 'w')\n",
    "    f.write(\"FileIndex,Category\\n\")\n",
    "    for i in range(len(speech.test_fnames)):\n",
    "        fname = speech.test_fnames[i]\n",
    "        f.write(fname + ',' + labels[i] + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(size, window, epochs, load_w2v):\n",
    "    print(\"Reading data\")\n",
    "    data_loc = \"data/\"\n",
    "    speech = read_files(data_loc, size, window, epochs, load_w2v)\n",
    "\n",
    "    print(\"Training classifier\")\n",
    "    cls = classify.train_classifier(speech.train_doc_vec, speech.trainy)\n",
    "\n",
    "    # cls = classify.semi_supervised_learning(cls, speech.train_doc_vec, speech.trainy, speech.unlabeled_doc_vec, speech.dev_doc_vec, speech.devy)\n",
    "\n",
    "    print(\"Evaluating\")\n",
    "    train_acc = classify.evaluate(speech.train_doc_vec, speech.trainy, cls)\n",
    "    dev_acc = classify.evaluate(speech.dev_doc_vec, speech.devy, cls)\n",
    "\n",
    "    print(\"Writing Kaggle pred file\")\n",
    "    write_pred_kaggle_file(cls, \"data/speech-pred.csv\", speech)\n",
    "    print(\"size: \" + str(size) + \"   window: \" + str(window) + \"   epochs: \" + str(epochs))\n",
    "    print(\"train_acc: \" + str(train_acc))\n",
    "    print(\"dev_acc: \" + str(dev_acc))\n",
    "    print(\"=================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "Reading data\n",
      "-- train data\n",
      "8240\n",
      "-- dev data\n",
      "1\n",
      "-- test data\n",
      "-- transforming data and labels\n",
      "trainX 생성 완료 shape: (8240, 18643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hrockk/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "/Users/Hrockk/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec 생성 완료: (18643, 1100)\n",
      "saved file: w2v_learned/w2v.p\n",
      "Training classifier\n",
      "Evaluating\n",
      "Writing Kaggle pred file\n",
      "size: 1100   window: 35   epochs: 120\n",
      "train_acc: 0.8523058252427185\n",
      "dev_acc: 1.0\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Hrockk/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # run(size, window, epochs)\n",
    "    print(\"=================================\")\n",
    "    size = 1100\n",
    "    window = 35\n",
    "    epochs = 120\n",
    "    load_w2v = False\n",
    "    run(size, window, epochs, load_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
